{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GALAXY' 'QSO' 'STAR']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run_ID</th>\n",
       "      <th>rerun_ID</th>\n",
       "      <th>cam_col</th>\n",
       "      <th>field_ID</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>MJD</th>\n",
       "      <th>fiber_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>0.634794</td>\n",
       "      <td>5812</td>\n",
       "      <td>56354</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>4518</td>\n",
       "      <td>301</td>\n",
       "      <td>5</td>\n",
       "      <td>119</td>\n",
       "      <td>0.779136</td>\n",
       "      <td>10445</td>\n",
       "      <td>58158</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.644195</td>\n",
       "      <td>4576</td>\n",
       "      <td>55592</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>4192</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>0.932346</td>\n",
       "      <td>9149</td>\n",
       "      <td>58039</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>8102</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>0.116123</td>\n",
       "      <td>6121</td>\n",
       "      <td>56187</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>116.742115</td>\n",
       "      <td>42.269971</td>\n",
       "      <td>22.84368</td>\n",
       "      <td>22.29973</td>\n",
       "      <td>20.75661</td>\n",
       "      <td>19.95866</td>\n",
       "      <td>19.47037</td>\n",
       "      <td>1402</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>0.476464</td>\n",
       "      <td>3666</td>\n",
       "      <td>55185</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>118.188167</td>\n",
       "      <td>44.085136</td>\n",
       "      <td>25.64453</td>\n",
       "      <td>22.74920</td>\n",
       "      <td>20.78390</td>\n",
       "      <td>19.89102</td>\n",
       "      <td>19.35816</td>\n",
       "      <td>1402</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>0.501545</td>\n",
       "      <td>6376</td>\n",
       "      <td>56269</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>123.109994</td>\n",
       "      <td>58.186645</td>\n",
       "      <td>19.60161</td>\n",
       "      <td>19.41832</td>\n",
       "      <td>19.41733</td>\n",
       "      <td>19.13950</td>\n",
       "      <td>19.05412</td>\n",
       "      <td>4264</td>\n",
       "      <td>301</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>1.854104</td>\n",
       "      <td>1873</td>\n",
       "      <td>54437</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>359.461825</td>\n",
       "      <td>28.348221</td>\n",
       "      <td>19.39042</td>\n",
       "      <td>17.85960</td>\n",
       "      <td>17.23222</td>\n",
       "      <td>16.95590</td>\n",
       "      <td>16.89005</td>\n",
       "      <td>4152</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>2803</td>\n",
       "      <td>54368</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>344.763611</td>\n",
       "      <td>14.038418</td>\n",
       "      <td>25.97670</td>\n",
       "      <td>21.18209</td>\n",
       "      <td>20.23094</td>\n",
       "      <td>19.89018</td>\n",
       "      <td>19.65660</td>\n",
       "      <td>1739</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>204</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>741</td>\n",
       "      <td>52261</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            alpha      delta         u         g         r         i  \\\n",
       "0      135.689107  32.494632  23.87882  22.27530  20.39501  19.16573   \n",
       "1      144.826101  31.274185  24.77759  22.83188  22.58444  21.16812   \n",
       "2      142.188790  35.582444  25.26307  22.66389  20.60976  19.34857   \n",
       "3      338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454   \n",
       "4      345.282593  21.183866  19.43718  17.58028  16.49747  15.97711   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "19995  116.742115  42.269971  22.84368  22.29973  20.75661  19.95866   \n",
       "19996  118.188167  44.085136  25.64453  22.74920  20.78390  19.89102   \n",
       "19997  123.109994  58.186645  19.60161  19.41832  19.41733  19.13950   \n",
       "19998  359.461825  28.348221  19.39042  17.85960  17.23222  16.95590   \n",
       "19999  344.763611  14.038418  25.97670  21.18209  20.23094  19.89018   \n",
       "\n",
       "              z  run_ID  rerun_ID  cam_col  field_ID  redshift  plate    MJD  \\\n",
       "0      18.79371    3606       301        2        79  0.634794   5812  56354   \n",
       "1      21.61427    4518       301        5       119  0.779136  10445  58158   \n",
       "2      18.94827    3606       301        2       120  0.644195   4576  55592   \n",
       "3      19.25010    4192       301        3       214  0.932346   9149  58039   \n",
       "4      15.54461    8102       301        3       137  0.116123   6121  56187   \n",
       "...         ...     ...       ...      ...       ...       ...    ...    ...   \n",
       "19995  19.47037    1402       301        4        70  0.476464   3666  55185   \n",
       "19996  19.35816    1402       301        4        84  0.501545   6376  56269   \n",
       "19997  19.05412    4264       301        6       155  1.854104   1873  54437   \n",
       "19998  16.89005    4152       301        1       103  0.000079   2803  54368   \n",
       "19999  19.65660    1739       301        3       204 -0.000178    741  52261   \n",
       "\n",
       "       fiber_ID  \n",
       "0           171  \n",
       "1           427  \n",
       "2           299  \n",
       "3           775  \n",
       "4           842  \n",
       "...         ...  \n",
       "19995       973  \n",
       "19996       695  \n",
       "19997       316  \n",
       "19998       396  \n",
       "19999       595  \n",
       "\n",
       "[20000 rows x 15 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('star_classification.csv')\n",
    "df[\"class\"].unique()\n",
    "df = df[0:20000]\n",
    "x = df.drop('class', axis=1)\n",
    "x = x.drop(\"obj_ID\", axis=1)\n",
    "x = x.drop(\"spec_obj_ID\", axis=1)\n",
    "y = df['class']\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "###\n",
    "theta = np.zeros( (15,) ) #15 attributes in the dataset\n",
    "iterations = 500\n",
    "alpha = 0.000002    #the learning rate\n",
    "###\n",
    "classes = {'GALAXY':0, 'QSO':1, 'STAR':2 }\n",
    "print(y.unique())\n",
    "x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost( X, y, theta ):\n",
    "  \n",
    "\n",
    "    m = X.shape[0]#number of training examples\n",
    "\n",
    "    J = 0\n",
    "\n",
    "    J = (1 / (2.0*m) ) * np.sum( ( ( np.dot(X,theta)-y)**2 ) )\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformYtoNum(y):\n",
    "    # Transforms String classes to integers\n",
    "    y_new = np.zeros((len(y)))\n",
    "    for i in range(len(y)):\n",
    "        y_new[i] = classes.get(y[i])\n",
    "\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Converting pd frames to numpy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "x_train = x_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling των τιμων καθως ορισμενες ιδιοτητες του dataset περιλαμβανουν τιμες με πολλα ψηφια και οδηγουσαν σε overflow με αποτελεσμα την παραγωγη theta με τιμες NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaler:\n",
      " [[ 1.69683592e+02 -2.08234967e+00  2.45212200e+01 ...  3.77400000e+03\n",
      "   5.52440000e+04  4.76000000e+02]\n",
      " [ 8.86939651e+00 -1.14569542e+00  2.48811000e+01 ...  7.86700000e+03\n",
      "   5.70030000e+04  9.57000000e+02]\n",
      " [ 1.34287502e+02  1.09339668e+01  2.01741600e+01 ...  2.67100000e+03\n",
      "   5.41410000e+04  6.30000000e+02]\n",
      " ...\n",
      " [ 1.48137883e+02  9.84109050e-01  1.79404800e+01 ...  2.67000000e+02\n",
      "   5.16080000e+04  5.34000000e+02]\n",
      " [ 5.58294316e+01  9.76439658e+00  1.77922400e+01 ...  2.67900000e+03\n",
      "   5.43680000e+04  2.87000000e+02]\n",
      " [ 1.89902619e+02  3.37795907e+01  2.49314200e+01 ...  3.97100000e+03\n",
      "   5.53220000e+04  4.50000000e+02]]\n",
      "After scaler:\n",
      " [[-0.08300705 -1.29521763  1.09045971 ... -0.43992811 -0.16558138\n",
      "   0.10628772]\n",
      " [-1.69235606 -1.24618879  1.25025044 ...  0.94950427  0.80190371\n",
      "   1.90063875]\n",
      " [-0.43723363 -0.6138831  -0.83968341 ... -0.81435858 -0.77225339\n",
      "   0.68077849]\n",
      " ...\n",
      " [-0.29862586 -1.13470492 -1.83146224 ... -1.6304337  -2.16545392\n",
      "   0.32265437]\n",
      " [-1.22240323 -0.67510384 -1.89728245 ... -0.81164286 -0.64739886\n",
      "  -0.59876913]\n",
      " [ 0.11933497  0.58196309  1.27259308 ... -0.3730534  -0.12267983\n",
      "   0.00929577]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_transformed = transformYtoNum(y_train)\n",
    "\n",
    "# Using StandardScaler for decimal values with many digits so OVERFLOW in calculations is prevented\n",
    "scaler=StandardScaler()\n",
    "x_train_scaler=scaler.fit_transform(x_train)\n",
    "print(\"Before scaler:\\n\", x_train)\n",
    "print(\"After scaler:\\n\", x_train_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακατω υπαρχει μια υλοποιηση του Gradient descent που κανει χρηση της computeCost(), δηλαδη της least square method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    \n",
    "    # Linear regression normal equation\n",
    "    theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    \n",
    "    m = len(y)  # Number of samples\n",
    "    cost_history = [] \n",
    "    print(theta.shape)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        #print(_)\n",
    "        # Calculate predictions\n",
    "        predictions = X @ theta\n",
    "        \n",
    "        # Compute the gradient of the cost function\n",
    "        gradient = (1 / m) * X.T @ (predictions - y)\n",
    "        #print(gradient)\n",
    "        \n",
    "        # Update theta\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "        # Compute the cost and save it\n",
    "        cost = computeCost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "[ 4.00867021e-06 -1.74075971e-05  1.25853836e-04  6.95232429e-05\n",
      " -2.68547094e-05 -8.61652518e-05 -1.12294853e-04 -6.93380590e-06\n",
      "  0.00000000e+00 -6.15957872e-07  2.35100227e-05 -1.26944013e-04\n",
      " -5.40961293e-05 -5.65898120e-05 -5.51917268e-06]\n",
      "(15,)\n",
      "[ 7.13971201e-06  2.49915983e-05 -4.79531965e-05  2.70140447e-05\n",
      "  9.83586167e-05  1.43409762e-04  1.57137240e-04  2.38779374e-05\n",
      "  0.00000000e+00 -1.31312444e-05 -9.69526558e-06  2.98853690e-04\n",
      "  1.10359186e-04  1.06088692e-04  2.69451101e-05]\n",
      "(15,)\n",
      "[-1.11483822e-05 -7.58400125e-06 -7.79006393e-05 -9.65372876e-05\n",
      " -7.15039074e-05 -5.72445101e-05 -4.48423877e-05 -1.69441315e-05\n",
      "  0.00000000e+00  1.37472023e-05 -1.38147571e-05 -1.71909677e-04\n",
      " -5.62630568e-05 -4.94988803e-05 -2.14259374e-05]\n",
      "[0.10957130353668909, 0.10957117850398136, 0.10957105347330534, 0.109570928444661, 0.10957080341804827, 0.10957067839346714, 0.10957055337091756, 0.10957042835039947, 0.10957030333191282, 0.10957017831545757, 0.10957005330103371, 0.10956992828864115, 0.10956980327827985, 0.1095696782699498, 0.10956955326365093, 0.1095694282593832, 0.10956930325714659, 0.109569178256941, 0.10956905325876642, 0.10956892826262284, 0.10956880326851015, 0.10956867827642833, 0.10956855328637738, 0.10956842829835717, 0.10956830331236773, 0.10956817832840898, 0.10956805334648091, 0.10956792836658343, 0.10956780338871652, 0.10956767841288013, 0.10956755343907422, 0.10956742846729874, 0.10956730349755367, 0.10956717852983894, 0.10956705356415451, 0.10956692860050031, 0.10956680363887636, 0.10956667867928258, 0.1095665537217189, 0.10956642876618533, 0.10956630381268177, 0.10956617886120822, 0.10956605391176459, 0.1095659289643509, 0.10956580401896707, 0.10956567907561304, 0.10956555413428877, 0.10956542919499425, 0.10956530425772941, 0.1095651793224942, 0.10956505438928861, 0.10956492945811254, 0.10956480452896598, 0.1095646796018489, 0.10956455467676122, 0.10956442975370292, 0.10956430483267397, 0.10956417991367431, 0.10956405499670385, 0.10956393008176263, 0.10956380516885056, 0.10956368025796759, 0.10956355534911368, 0.10956343044228878, 0.10956330553749287, 0.1095631806347259, 0.10956305573398782, 0.10956293083527859, 0.10956280593859814, 0.10956268104394645, 0.10956255615132347, 0.10956243126072918, 0.1095623063721635, 0.10956218148562641, 0.10956205660111784, 0.10956193171863778, 0.10956180683818613, 0.10956168195976293, 0.10956155708336805, 0.10956143220900154, 0.10956130733666325, 0.10956118246635321, 0.10956105759807133, 0.1095609327318176, 0.10956080786759198, 0.10956068300539441, 0.10956055814522483, 0.10956043328708322, 0.10956030843096955, 0.10956018357688371, 0.10956005872482572, 0.10955993387479553, 0.10955980902679306, 0.10955968418081828, 0.10955955933687118, 0.10955943449495167, 0.10955930965505975, 0.10955918481719534, 0.1095590599813584, 0.1095589351475489, 0.10955881031576677, 0.10955868548601198, 0.10955856065828452, 0.10955843583258432, 0.10955831100891131, 0.1095581861872655, 0.10955806136764677, 0.10955793655005515, 0.10955781173449057, 0.10955768692095295, 0.1095575621094423, 0.10955743729995855, 0.10955731249250167, 0.1095571876870716, 0.1095570628836683, 0.10955693808229172, 0.10955681328294181, 0.10955668848561857, 0.10955656369032192, 0.1095564388970518, 0.10955631410580821, 0.10955618931659106, 0.10955606452940035, 0.10955593974423601, 0.109555814961098, 0.10955569017998629, 0.10955556540090079, 0.1095554406238415, 0.10955531584880837, 0.10955519107580135, 0.1095550663048204, 0.10955494153586545, 0.1095548167689365, 0.10955469200403349, 0.10955456724115636, 0.10955444248030506, 0.10955431772147958, 0.10955419296467986, 0.10955406820990585, 0.10955394345715751, 0.10955381870643478, 0.10955369395773762, 0.10955356921106604, 0.10955344446641994, 0.10955331972379928, 0.10955319498320402, 0.10955307024463413, 0.10955294550808956, 0.10955282077357026, 0.10955269604107619, 0.1095525713106073, 0.10955244658216355, 0.1095523218557449, 0.1095521971313513, 0.10955207240898271, 0.10955194768863909, 0.10955182297032039, 0.10955169825402654, 0.10955157353975756, 0.10955144882751335, 0.10955132411729387, 0.1095511994090991, 0.109551074702929, 0.10955094999878352, 0.10955082529666259, 0.1095507005965662, 0.10955057589849428, 0.10955045120244677, 0.10955032650842367, 0.10955020181642493, 0.10955007712645047, 0.1095499524385003, 0.10954982775257434, 0.10954970306867255, 0.10954957838679487, 0.10954945370694129, 0.10954932902911174, 0.1095492043533062, 0.1095490796795246, 0.10954895500776692, 0.10954883033803306, 0.10954870567032306, 0.10954858100463682, 0.10954845634097433, 0.10954833167933552, 0.10954820701972033, 0.10954808236212878, 0.10954795770656076, 0.10954783305301626, 0.1095477084014952, 0.10954758375199761, 0.10954745910452336, 0.10954733445907247, 0.10954720981564486, 0.1095470851742405, 0.10954696053485934, 0.10954683589750133, 0.10954671126216646, 0.10954658662885465, 0.10954646199756586, 0.10954633736830006, 0.1095462127410572, 0.10954608811583724, 0.10954596349264012, 0.10954583887146582, 0.10954571425231426, 0.10954558963518542, 0.10954546502007927, 0.10954534040699576, 0.10954521579593483, 0.10954509118689645, 0.10954496657988054, 0.1095448419748871, 0.10954471737191608, 0.10954459277096742, 0.10954446817204108, 0.10954434357513704, 0.10954421898025522, 0.1095440943873956, 0.10954396979655809, 0.10954384520774273, 0.1095437206209494, 0.1095435960361781, 0.10954347145342876, 0.10954334687270136, 0.10954322229399584, 0.10954309771731215, 0.10954297314265027, 0.10954284857001013, 0.10954272399939169, 0.10954259943079493, 0.1095424748642198, 0.1095423502996662, 0.10954222573713418, 0.10954210117662362, 0.1095419766181345, 0.1095418520616668, 0.10954172750722045, 0.10954160295479543, 0.10954147840439163, 0.10954135385600909, 0.1095412293096477, 0.10954110476530748, 0.10954098022298833, 0.10954085568269024, 0.10954073114441314, 0.10954060660815702, 0.10954048207392178, 0.10954035754170745, 0.10954023301151393, 0.10954010848334121, 0.10953998395718922, 0.1095398594330579, 0.10953973491094728, 0.10953961039085724, 0.10953948587278776, 0.10953936135673882, 0.10953923684271033, 0.10953911233070229, 0.10953898782071465, 0.10953886331274734, 0.10953873880680032, 0.1095386143028736, 0.10953848980096705, 0.10953836530108066, 0.10953824080321443, 0.10953811630736827, 0.10953799181354212, 0.109537867321736, 0.1095377428319498, 0.10953761834418352, 0.10953749385843711, 0.1095373693747105, 0.10953724489300366, 0.10953712041331656, 0.10953699593564917, 0.10953687146000138, 0.10953674698637321, 0.10953662251476459, 0.10953649804517547, 0.10953637357760583, 0.1095362491120556, 0.10953612464852476, 0.10953600018701325, 0.10953587572752103, 0.10953575127004804, 0.10953562681459426, 0.10953550236115965, 0.10953537790974416, 0.10953525346034772, 0.10953512901297031, 0.10953500456761191, 0.10953488012427241, 0.10953475568295183, 0.1095346312436501, 0.10953450680636717, 0.10953438237110298, 0.10953425793785754, 0.10953413350663077, 0.10953400907742263, 0.10953388465023306, 0.10953376022506207, 0.10953363580190954, 0.1095335113807755, 0.10953338696165985, 0.10953326254456257, 0.10953313812948362, 0.10953301371642295, 0.10953288930538053, 0.10953276489635627, 0.10953264048935019, 0.1095325160843622, 0.10953239168139227, 0.10953226728044034, 0.10953214288150641, 0.10953201848459039, 0.10953189408969226, 0.10953176969681198, 0.1095316453059495, 0.10953152091710476, 0.10953139653027773, 0.10953127214546837, 0.10953114776267664, 0.10953102338190247, 0.10953089900314586, 0.10953077462640672, 0.10953065025168501, 0.10953052587898074, 0.10953040150829381, 0.10953027713962418, 0.10953015277297183, 0.10953002840833674, 0.1095299040457188, 0.10952977968511801, 0.10952965532653432, 0.10952953096996765, 0.10952940661541802, 0.10952928226288534, 0.10952915791236958, 0.1095290335638707, 0.10952890921738864, 0.10952878487292336, 0.10952866053047486, 0.10952853619004305, 0.10952841185162789, 0.10952828751522933, 0.10952816318084736, 0.10952803884848189, 0.10952791451813292, 0.1095277901898004, 0.10952766586348424, 0.10952754153918445, 0.10952741721690096, 0.10952729289663372, 0.10952716857838271, 0.10952704426214789, 0.10952691994792918, 0.10952679563572656, 0.10952667132553998, 0.10952654701736941, 0.1095264227112148, 0.10952629840707606, 0.10952617410495323, 0.10952604980484619, 0.10952592550675495, 0.10952580121067945, 0.10952567691661963, 0.10952555262457547, 0.10952542833454691, 0.1095253040465339, 0.10952517976053641, 0.10952505547655439, 0.10952493119458782, 0.10952480691463662, 0.10952468263670076, 0.10952455836078019, 0.10952443408687487, 0.10952430981498477, 0.10952418554510984, 0.10952406127725002, 0.1095239370114053, 0.1095238127475756, 0.10952368848576088, 0.10952356422596113, 0.10952343996817628, 0.10952331571240624, 0.10952319145865107, 0.10952306720691066, 0.10952294295718497, 0.10952281870947397, 0.1095226944637776, 0.10952257022009583, 0.10952244597842863, 0.1095223217387759, 0.10952219750113765, 0.10952207326551383, 0.10952194903190439, 0.10952182480030928, 0.10952170057072844, 0.10952157634316187, 0.10952145211760948, 0.10952132789407126, 0.10952120367254715, 0.10952107945303712, 0.10952095523554113, 0.10952083102005909, 0.10952070680659101, 0.1095205825951368, 0.10952045838569648, 0.10952033417826994, 0.10952020997285716, 0.1095200857694581, 0.10951996156807274, 0.109519837368701, 0.10951971317134285, 0.10951958897599824, 0.10951946478266711, 0.10951934059134948, 0.10951921640204523, 0.10951909221475437, 0.10951896802947682, 0.10951884384621255, 0.10951871966496153, 0.10951859548572371, 0.10951847130849901, 0.10951834713328744, 0.10951822296008895, 0.10951809878890345, 0.10951797461973095, 0.10951785045257134, 0.10951772628742466, 0.10951760212429082, 0.10951747796316975, 0.10951735380406147, 0.10951722964696589, 0.10951710549188297, 0.10951698133881269, 0.10951685718775499, 0.10951673303870982, 0.10951660889167715, 0.10951648474665691, 0.1095163606036491, 0.10951623646265363, 0.10951611232367048, 0.10951598818669965, 0.10951586405174099, 0.10951573991879456, 0.10951561578786026, 0.10951549165893806, 0.10951536753202791, 0.10951524340712979, 0.10951511928424364, 0.10951499516336939, 0.10951487104450705, 0.1095147469276565, 0.10951462281281779, 0.10951449869999082, 0.10951437458917555, 0.10951425048037194, 0.10951412637357996, 0.10951400226879954, 0.10951387816603064, 0.10951375406527328, 0.1095136299665273, 0.10951350586979276, 0.10951338177506956, 0.10951325768235769, 0.10951313359165707, 0.10951300950296769, 0.10951288541628947, 0.10951276133162241, 0.10951263724896643, 0.10951251316832152, 0.10951238908968759, 0.10951226501306464, 0.1095121409384526, 0.10951201686585141, 0.10951189279526109, 0.10951176872668154, 0.10951164466011276, 0.10951152059555465, 0.1095113965330072, 0.10951127247247038, 0.10951114841394412, 0.10951102435742838, 0.10951090030292311, 0.1095107762504283, 0.10951065219994388, 0.10951052815146979, 0.10951040410500602, 0.10951028006055251, 0.10951015601810923, 0.10951003197767613, 0.10950990793925315, 0.10950978390284026, 0.10950965986843741, 0.10950953583604459, 0.10950941180566168, 0.10950928777728872, 0.10950916375092558]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test_transformed = transformYtoNum(y_test)\n",
    "#print(x_train.shape)\n",
    "theta_array = []\n",
    "for model in range(3):\n",
    "    y_binary = (y_train_transformed == model).astype(int)\n",
    "    theta_temp, history = gradient_descent(x_train_scaler, y_binary, theta, alpha, iterations)\n",
    "    #theta_temp = train_linear_regression(x_train, y_binary)\n",
    "    print(theta_temp)\n",
    "    theta_array.append(theta_temp)\n",
    "\n",
    "print(history)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στον παρακατω κωδικα εχει χρησιμοποιηθει μια μεθοδος classification με βαση το μεσο ορο των προβλεψεων καθε ενος μοντελου (3 μοντελα, ενα για καθε κατηγορια), ωστοσο το ποσοστο του accuracy εξακολουθει να μην ειναι καλο λογω της φυσης του linear regression που δεν ειναι καταλληλο για classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.02024547  5.73487036 -2.71462488]\n",
      " [-3.8979167   7.47298931 -3.57507261]\n",
      " [-3.19342955  6.0774923  -2.88406275]\n",
      " [-3.57637964  6.81242968 -3.23605004]\n",
      " [-3.57951315  6.83039593 -3.25088278]]\n",
      "Column means:\n",
      " [-3.44810462  6.58289519 -3.13479057]\n",
      "[0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
      "Accuracy: 0.13735\n"
     ]
    }
   ],
   "source": [
    "scores = np.array([predict(x_test, theta) for theta in theta_array]).T  # shape (n_samples, num_classes)\n",
    "\n",
    "print(scores[0:5])\n",
    "# Choose the class with the highest predicted score\n",
    "predicted_classes = np.argmax(scores, axis=1)\n",
    "\n",
    "column_means = np.mean(scores, axis=0)\n",
    "print(\"Column means:\\n\", column_means)\n",
    "\n",
    "def predict_by_mean(scores, column_means):\n",
    "\n",
    "    predicted_classes = []\n",
    "    for i in range(scores.shape[0]):\n",
    "\n",
    "        for j in range(3):\n",
    "            if scores[i][j] > column_means[j]:\n",
    "                predicted_classes.append(j)\n",
    "                break\n",
    "    \n",
    "    return predicted_classes\n",
    "        \n",
    "#print(predicted_classes)\n",
    "\n",
    "predicted_classes_by_mean = predict_by_mean(scores, column_means)\n",
    "print(predicted_classes_by_mean)\n",
    "\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(y_test_transformed)):\n",
    "    sum += 1 if y_test_transformed[i] == predicted_classes_by_mean[i] else 0\n",
    "\n",
    "print(\"Accuracy:\", sum/20000)\n",
    "#print(y_binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
